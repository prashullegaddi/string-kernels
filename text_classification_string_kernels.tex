\documentclass[10pt]{beamer}
\usepackage{latexsym}
\usepackage[longend]{algorithm2e}
\usepackage{wasysym}

\mode<presentation>
\usetheme{Frankfurt}
%\usecolortheme{beetle} % Beamer Color Theme
\usecolortheme{rose}
\usefonttheme{serif}


\title{Text Classification using String Kernels}
\author{Prashant Ullegaddi\\ 200807013\\
	\href{mailto:prashant.ullegaddi@research.iiit.ac.in}{\texttt{prashant.ullegaddi@research.iiit.ac.in}}}
\date{\today}

\begin{document}
\maketitle

\begin{frame}
	\frametitle{Outline.}
		\begin{itemize}
			\item The Problem.
			\item What is a String Kernel?
			\item Salient Features of String Kernel.
			\item Previous Work.
			\item Basic Notations and Definitions.
			\item Feature Space of String Kernel.
			\item Basic Algorithm -- Kernel as an Inner Product.
			\item An Example.
			\item Normalizing the Kernel Value.
			\item Key to Recursive Computation.
			\item Recursive Computation of String Kernel - Kernel Trick.
			\item SSK (String Subsequence Kernel) Algorithm.
			\item SSK with an Example.
			\item Discussion on SSK.
			\item References.
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The Problem.}
		\begin{itemize}
		\item Given two text document, how to compare them?
		\item Are two text documents similar to each other?
		\item If yes, what is the similarity score?
		\item This helps in classifying/categorizing the large text collections.
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{What is a String Kernel?}
	\begin{itemize}
		\item A string kernel between two strings is an inner product in the feature space generated by all subsequences of 
				length n.
		\item A subsequence is any ordered sequence of n characters occurring in the text though need not occur contiguously.
		\item E.g., subsequence 'car' is occurring in '\textbf{car}d' contiguously, but non-contiguously in '\textbf{c}ust\textbf{ar}d'.
		\item Main idea: Compare two text documents by means of the substrings they contain in common: the more substrings in common, the more similar they are.
	\end{itemize}
	% fill in some explanation
\end{frame}

\begin{frame}
	\frametitle{Salient Features of String Kernel.}
	\begin{itemize}
		\item String Kernel considers non-contiguous occurrences of subsequences as well but they are weighted less compared to continguous occurrences.
		\item It considers the ordering of the word information in text.\\
				E.g. the two strings: \\\textbf{"The interest rate goes up, US dollar goes down."} and \textbf{ \\"The interest rate goes down, US dollar goes up."} \\
				These two will be identified as different strings by String kernel where as traditional methods like Word Kernels miss such ordering information.
		\item Doesn't take any semantic information about text, only works at low-level (character level matching). Boon or bane \smiley.
		\item E.g. computer and desktop are treated differently.
	\end{itemize}
\end{frame}

\begin{frame}
   \frametitle{Previous Work.}
	   \begin{itemize}
	      \item Bag-of-Words (BOW) approach (also known as Word Kernel approach)
			\begin{itemize}
				\item Breaks the documents into words
				\item Words occurring more than a threshold are considered features
			\end{itemize}

			\item N-grams approach (N-gram  kernel)
			\begin{itemize}
				\item	Same as BOW, but instead of words here n-grams are considered.
				\item E.g., 2-grams of CUSTARD are CU, US, ST, TA, AR, and RD. 
				\item Note: It's a language-independent model.
			\end{itemize}
			
			\item String Kernel approach is compared to these two approaches.
	   \end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Basic Notations and Definitions.}
	\begin{itemize}
		\item Let $\sum$ be the alphabet.
		\item A string $s$ has length $|s|$ and denoted by $s$ = $s_1...s_{|s|}$.
		\item $st$ denotes concatenation of strings $s$ and $t$.
		\item The string $s[i:j]$ is the substring $s_i . . . s_j$ of $s$.
		\item A string $u$ is a subsequence of $s$ if $u$ occurs in $s$ though need not occur contiguously! That is there
					exist indices $\textbf{i}$ = $(i_1,...,i_{|u|})$ \\ with $1$ $\leq$ $i_1$ $<$ ... $<$ $i_{|u|}$ $\leq$ $|s|$
					such that $u$ = $s[\textbf{i}]$.
		\item Length $l(\textbf{i})$ of subsequence $u$ in string $s$ is $i_{|u|} - i_1 + 1$, i.e., the window the subsequence 
					spans in $s$.
		\item $\sum^n$ denotes all strings of length $n$ from alphabet $\sum$.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Feature Space of String Kernel.}
	\begin{itemize}
		\item String Kernels work in $|\sum|^n$ dimensional space.
		\item Feature space is $F_n$ = $\mathbb{R}^{|\sum|^n}$.
		\item Feature value corresponding to some feature $u$ (for any $u$ $\in$ $\sum^n$) for string $s$ is given by\\
		\begin{equation*}
				\phi_u(s) = \sum_{u=s[\textbf{i}]} \lambda^{l(\textbf{i})} \qquad \textrm{ with } \quad \lambda \in (0, 1). 
		\end{equation*}
		\item E.g. consider the string, $s$ = \textit{custard} and feature $u$ = \textit{car}. The subsequence 'car' has a 
				value $\lambda^{6-1+1}$ (= $\lambda^6$) in the string 'custard' since $u$ = $s[\textbf{i}]$ for $\textbf{i}$ = (1, 5, 6).
				Similarly its value in 'card' is $\lambda^{3-1+1}$ = $\lambda^3$, so that $\phi_{car}(card) > \phi_{car}(custard)$. 
		\item This shows that contiguous occurrences weighted high compared to non-contiguous occurrences of the same subsequence.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{String Kernel as an Inner Product.}
	For given two string $s$ and $t$ and length $n$, we can compute string kernel as inner product of feature vectors
	$\Phi$ of strings $s$ and $t$. Thus we have,
	\centering
	\begin{eqnarray*}
	K_n(s,t) & = & \langle \Phi(s), \Phi(t) \rangle \\
				& = & \sum_{u \in \sum^n} \phi_u(s).\phi_u(t) \\
				& =  &\sum_{u \in \sum^n} \sum_{\textbf{i}:u=s[\textbf{i}]} \lambda^{l(\textbf{i})} 
			\sum_{\textbf{j}:u=t[\textbf{j}]} \lambda^{l(\textbf{j})} \\
			& = & \sum_{u \in \sum^n} \sum_{\textbf{i}:u=s[\textbf{i}]}  \sum_{\textbf{j}:u=t[\textbf{j}]} \lambda^{l(\textbf{i}) + l(\textbf{j})}
	\end{eqnarray*}
\end{frame}

% The algorithm
\begin{frame}
	\frametitle{Basic Algorithm -- Kernel as an Inner Product}
	\begin{itemize}
		\item [Step 1:] Enumerate all substrings of length n. 
		\item [Step 2:] Compute the feature vectors (where each feature is string enumerated above) for given two documents.
		\item [Step 3:] Compute the similarity between the documents as inner product between the feature vectors ($K_n(s,t)$ = $\langle\Phi(s),\Phi(t)\rangle$).
	\end{itemize}

	NOTE:
	\begin{enumerate}
		\item Efficiency is ${O(|\sum|^n)}$, just to compute the features!
		\item With increase in $n$, computing all features becomes computationally impractical.
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{An Example.}
	\begin{itemize}
		\item	Let \textit{cat, car, bat,} and \textit{bar} be four documents. 
		\item The features (for n = 2) for these documents are \textit{ c-a, c-t, b-a, b-t, a-t, a-r, c-r}, and \textit{b-r}. 
		\item The feature vector for each of the documents is given below:
	\begin{table}[h]
	\caption{Feature vectors for the four documents.} %title of the table
	\centering                          % centering table
	\begin{tabular}{c c c c c c c c c c}  % creating 10 columns
	\hline\hline                        %inserting double-line
	\ & c-a & c-t & a-t & b-a & b-t & c-r & a-r & b-r & others\\ % inserts table heading
	\hline                               % inserts single-line
	$\phi$(cat)  & $\lambda^2$ & $\lambda^3$ & $\lambda^2$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$\\ % Entering row contents
	$\phi$(car)  & $\lambda^2$ & $0$ & $0$ & $0$ & $0$ & $\lambda^3$ & $\lambda^2$ & $0$ & $0$ \\ % Entering row contents
	$\phi$(bat)  & $0$ & $0$ & $\lambda^2$ & $\lambda^2$ & $\lambda^3$ & $0$ & $0$ & $0$ & $0$  \\ % Entering row contents
	$\phi$(bar)  & $0$ & $0$ & $0$ & $\lambda^2$ & $0$ & $0$ & $\lambda^2$ & $\lambda^3$ & $0$ \\ % Entering row contents
	\hline                          % inserts single-line
	\end{tabular}
	\label{tab:hresult}
	\end{table}

	\item Applying equation (2), we have $K_2(cat,bat) = \lambda^2 . \lambda^2 = \lambda^4$
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Normalizing the Kernel Value.}
	N\^ormalized kernel value is given by
	\begin{equation}
		\hat{K}_n(s,t) = \frac{K_n(s,t)}{\sqrt{K_n(s,s) K_n(t,t)}}
	\end{equation}

	For the example above the normalized kernel value is \\
	\begin{eqnarray*}
		\hat{K}_2(cat,bat) & = & \frac{K_2(cat,bat)}{\sqrt{K_2(cat,cat) K_2(bat,bat)}}\\
			 & = & \frac{\lambda^4}{\sqrt{(2\lambda^4+\lambda^6)(2\lambda^4+\lambda^6)}}\\
		\hat{K}_2(cat,bat) & = & \frac{1}{2+\lambda^2}
	\end{eqnarray*}
\end{frame}

\begin{frame}
	\frametitle{Key to Recursive Computation.}
	Suppose we know the value of the kernel for string $s$ and $t$, then how can we use this value to compute kernel for $sx$ and $t$
	where $x$ is any symbol such that $x \in \sum$?
	\begin{itemize}
		\item Subsequences common to strings $s$ and $t$ are also common to string $sx$ and $t$.
		\item Also consider new matching subsequences ending in $x$ in $t$ and whose $(n-1)$ suffix is in $s$.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Recursive Computation of String Kernel -- Kernel Trick.}
	The recursion for computing the Subsequence Kernel:
	\centering
	\begin{eqnarray}
	K'_0(s,t) & = & 1 \qquad  \textit{for all s, t} \label{one}\\
	K'_i(s,t) & = & 0 \qquad \textit{if min($|s|$,$|t|$) $<$ i} \label{two}  \\
	K'_i(sx,t) & = & \lambda K'_i(s,t) + \sum_{j:t_j=x}K'_{i-1}(s,t[1:(j-1)]) \lambda^{|t|-j+2} \label{three}  \\
	K_i(s,t) & = & 0 \qquad \textit{if min($|s|$,$|t|$) $<$ i, for $i = 1,...,n-1$} \label{four} \\
	K_n(sx,t) & = & K_n(s,t) + \lambda^2 \sum_{j:t_j=x}K'_{n-1}(s,t[1:j-1])\label{five} 
	\end{eqnarray}
	\centering

	\begin{itemize}
		\item Uses recursion to solve the problem without explicitly computing features (the subsequences of length n).
		\item Uses Dynamic Programming to store intermediate results to speed up the computation.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\small{SSK($s[1:p]$, $t[1:q]$, $n$, $\lambda$)}\\ \scriptsize{/* $s$ and $t$ are strings of lengths $p$, and $q$, $n$ is subsequences length, $\lambda$ is the decay factor.*/}}
	\begin{algorithm}[H]
	  	\SetLine
		\SetKwComment{Comment}{}{}
		\dontprintsemicolon
%	  	\KwData{$s[1:p]$, $t[1:q]$, $n$, $\lambda$}
%	 	\KwResult{StringKernel between strings $s$ and $t$}
		$K'_{prev}(0:q,0:p)$ $\gets$ $1$ \quad \Comment*[h]{  /* eq.(2) */}\;

		\For{$l$ $\gets$ $1$ to $n-1$}{
			\For{$i$ $\gets$ $0$ to $q$}{
				\For{$j$ $\gets$ $0$ to $p$}{
					\eIf{min($i,j$) $< l$}{
						$K'_{next}(i,j)$ $\gets$ $0$ \quad \Comment*[h]{  /* eq.(3) */}\;
						}{
						$K'_{next}(i,j)$ $\gets$ $\lambda$ $K'_{next}$($i$,$j-1$)  \Comment*[h]{ /* 1st term, eq.(4)*/}\;
						\For(\tcc*[f]{2nd term of eq.(4)}){$k$ $\gets$ $1$ to $i$}{
							\If{$s[j]$ = $t[k]$}{
								\footnotesize{$K'_{next}(i,j)$ $\gets$ $K'_{next}(i,j)$ $+$ $\lambda^{i-k+2}$ $K'_{prev}$($k-1$,$j-1$)\;}
								}
							}
						}

					}
				}
			$K'_{prev}$ $\gets$ $K'_{next}$\;
			}			
		\end{algorithm}
\end{frame}

\begin{frame}
	\frametitle{SSK -- String Subsequence Kernel (...continued)}
	\SetKwComment{Comment}{}{}
	\dontprintsemicolon

	\begin{algorithm}[H]
		\SetLine
		\For{$i$ $\gets$ $0$ to $q$}{
      	\For{$j$ $\gets$ $0$ to $p$}{
			\eIf{min($i,j$) $< n$}{
				$K_n(i,j)$ $\gets$ $0$ \quad \Comment*[h]{/* eq.(5)*/}\;
				}{
				$K_n(i,j)$ $\gets$ $K_n$($i$,$j-1$) \quad \Comment*[h]{ /* 1st term, eq.(6)*/}\;
				\For(\tcc*[f]{2nd term of eq.(6)}){$k$ $\gets$ $1$ to $i$}{
					\If{$s[j]$ = $t[k]$}{
						$K_n(i,j)$ $\gets$ $K_n(i,j)$ $+$ $\lambda^2$ $K'_{next}$($k-1$,$j-1$)\;
						}																																													                     }
		      }
       	}
		}
		\Return $K_n(q,p)$\;
	\end{algorithm}
\end{frame}

\begin{frame}
	\frametitle{SSK with an Example.}
	Let $s$ = \textit{cat}, and $t$ = \textit{bat} be the two strings. $K_2$($s$, $t$) can be computed as follows starting with $K'_0$.
	\begin{table}[h]
   \caption{For i = 0} %title of the table
	\begin{tabular}{| c | c | c | c | c |}  % creating 5 columns
	\hline
	$K'_0$ & $\epsilon$ & $c$ & $a$ & $t$ \\% inserts table heading
	\hline\hline
	$\epsilon$ & $1$ & $1$ & $1$ & $1$\\ \hline
	$b$ & $1$ & $1$ & $1$ & $1$\\ \hline
	$a$ & $1$ & $1$ & $1$ & $1$\\ \hline
	$t$ & $1$ & $1$ & $1$ & $1$\\ \hline							   
	\hline
	\end{tabular}
	\label{tab:hresult}
	\end{table}
\end{frame}

\begin{frame}
	\frametitle{SSK with an Example (...continued)}

	\begin{table}[h]
   \caption{For i = 1.} %title of the table
	\begin{tabular}{| c | c | c | c | c |}  % creating 5 columns
	\hline
	$K'_1$ & $\epsilon$ & $c$ & $a$ & $t$ \\  % inserts table heading
	\hline\hline
	$\epsilon$ & $0$ & $0$ & $0$ & $0$\\ \hline
	$b$ & $0$ & $0$ & $0$ & $0$\\ \hline   
	$a$ & $0$ & $0$ & $\lambda^2$ & $\lambda^3$\\ \hline
	$t$ & $0$ & $0$ & $\lambda^3$ & $\lambda^2+\lambda^4$\\ \hline
	\hline
	\end{tabular}
	\label{tab:hresult}
	\end{table}

	E.g. let us compute the value for cell $K'_1$(ca, ba). 
	\begin{eqnarray*}
		K'_1(ca, ba) & = & \lambda K'_1(c,ba) + \lambda^{|ba| - 2 + 2} K'_0(c, b)\\
			& = & \lambda . 0 + \lambda^{2 - 2 + 2} . 1\\
			& = & \lambda^2 \\
	\end{eqnarray*}
\end{frame}

\begin{frame}
	\frametitle{SSK with an Example (...continued)}
	\begin{table}[h]
   \caption{For last recursion to compute Kernel.} %title of the table
	\begin{tabular}{| c | c | c | c | c |}  % creating 5 columns
	\hline
	$K_2$ & $\epsilon$ & $c$ & $a$ & $t$ \\  % inserts table heading
	\hline\hline
	$\epsilon$ & $0$ & $0$ & $0$ & $0$\\ \hline
	$b$ & $0$ & $0$ & $0$ & $0$\\ \hline   
	$a$ & $0$ & $0$ & $0$ & $0$\\ \hline
	$t$ & $0$ & $0$ & $0$ & $\lambda^4$\\ \hline
	\hline
	\end{tabular}
	\label{tab:hresult}
	\end{table}

	\begin{eqnarray*}
		K_2(ca, ba) & = & K_2(c,ba) + \lambda^2 K'_1(c, b)\\
			& = & 0 + \lambda^2 . 0 = 0 \textrm{, and}
	\end{eqnarray*}
	\begin{eqnarray*}
      K_2(cat, bat) & = & K_2(ca,bat) + \lambda^2 K'_1(ca, bb)\\
        & = & 0 + \lambda^2 . \lambda^2 = \lambda^4
   \end{eqnarray*}
	The value of $K_2$(s,t) is given by the value in the cell ($|t|$, $|s|$) in the table $K_2$

\end{frame}

\begin{frame}
	\frametitle{Discussion on SSK.}
	\begin{itemize}
		\item The efficiency of SSK to compute $K_n(s,t)$ is $O(n|s||t|^2)$.
		\item Improvment can be made by looking at the repeated k-loop in the algorithm in computation. This can be eliminated by
				computing it and storing it for later use to compute $K'_i$.
		\item Define 
		\begin{equation}
			K''_i(sx,t) = \sum_{j:t_j=x}K'_{i-1}(s,t[1:j-1]) \lambda^{|t|-j+2}
		\end{equation}
		\item $K'_i(sx,t)$ can be computed as follows:
		\begin{equation}
			K'_i(sx,t) = \lambda K'_i(s,t) + K''_i(sx,t)
		\end{equation}
		\item With this change, a loop on $|t|$ is eliminated, making the efficiency $O(n|s||t|)$.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Approximation of SSK on Large Datasets}
	\begin{itemize}
		\item Since cost of computation of SSK is linear in document lengths, for large datasets computing SSK can still be very expensive.
				Hence an approximation of SSK is desired.
		\item An approximation method is specified here:
		\begin{enumerate}
			\item Pick k most occurring n-grams from the first N documents in the collection. Call this set $T_n$.
			\item For every document $d$, compute SSK between $d$ and $t$ for all $t \in T_n$. This computation must be less expensive as one of the strings is shorter.
			\item Find the inner product between documents with SSK computed above. This forms the new approximation to SSK.
		\end{enumerate}
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{New Approximated SSK.}
	\begin{equation}
		\tilde{K}_n(s,t) = \sum_{z \in T_n} \hat{K}_n(s,z) \hat{K}_n(z,t) 
	\end{equation}
		where $T_n$ is the set of all most occurring $n$-grams in the data set $D$ with $s,t$ $\in$ $D$.
\end{frame}

\begin{frame}
	\frametitle{References.}
	\begin{enumerate}
		\item Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, Chris Watkins, \textit{Text Classification using String Kernels}, JMLR 2001.
		\item Huma Lodhi, John Shawe-Taylor, Nello Cristianini, Chris Watkins, \textit{Text Classification using String Kernels}, NIPS 2000.
		\item Nicola Cancedda, Eric Gaussier, Cyril Goutte, Jean-Michel Renders, \textit{Word Sequence Kernels}, Appendix A. Recursive Formulation of Sequence Kernels, JMLR 2003.
		\item John Shawye-Taylor and Nello Christianini, \textit{Kernel Methods for Pattern Analysis}, Chapter 11 on \textit{Kernels for Structured Data: Strings, trees etc.}
	\end{enumerate}
\end{frame}

\begin{frame}
	\begin{center} \Huge{Thank You!} \end{center}
\end{frame}

\end{document}
